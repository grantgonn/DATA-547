{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 (20 points) \n",
    "\n",
    "Please state **True** or **False** for the below statements.\n",
    "\n",
    "- (a) Word embeddings represent words as high-dimensional sparse vectors. \n",
    "\n",
    "- (b) Pre-trained word embeddings like Word2Vec and GloVe can be fine-tuned for specific tasks. \n",
    "\n",
    "- (c) Word embeddings can capture the semantic similarity between words, such as `king` and `queen`. \n",
    "\n",
    "- (d) GloVe embeddings are generated using a predictive model based on the local context of words. \n",
    "\n",
    "- (e) Word embeddings are always static and cannot change during training on a specific task. \n",
    "\n",
    "- (f) Recurrent neural networks (RNNs) are primarily designed to process data in which the order of the data points does not matter. \n",
    "\n",
    "- (g) An RNN can suffer from the vanishing gradient problem during training, making it difficult to learn long-term dependencies. \n",
    "\n",
    "- (h) LSTM networks include a mechanism called gates that help regulate the flow of information. \n",
    "\n",
    "- (i) LSTM networks are immune to the vanishing gradient problem and never experience it during training. \n",
    "\n",
    "- (j) Gated Recurrent Units (GRUs) use fewer gates than Long Short-Term Memory (LSTM) networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. False\n",
    "\n",
    "b. True\n",
    "\n",
    "c. True\n",
    "\n",
    "d. False\n",
    "\n",
    "e. False\n",
    "\n",
    "f. False\n",
    "\n",
    "g. True\n",
    "\n",
    "h. True\n",
    "\n",
    "i. False\n",
    "\n",
    "j. True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 (5 points)\n",
    "\n",
    "Explain how increasing the depth of a Convolutional Neural Network (CNN) improves its ability to learn complex features in images. What challenges might arise from increasing the depth, and how can they be addressed? Please, be specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improving the depth can help the model learn features that were built on previous layers. Increasing the number of layers allows the network to build off the simple features learned in the early layers and recognize the finer details of the image. However, overfitting can be an issue if too many layers are added. This can be addressed by using dropout and/or adding diversity with data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 (4 points)\n",
    "\n",
    "Which of the following is a primary advantage of using a pre-trained CNN for image classification tasks?\n",
    "\n",
    "- (a) It guarantees perfect classification accuracy.\n",
    "- (b) It eliminates the need for labeled data in the new task.\n",
    "- (c) It reduces training time by leveraging features learned on a large dataset. \n",
    "- (d) It avoids the risk of overfitting entirely.\n",
    "- (e) All of the above.\n",
    "- (f) None of the above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 (4 points)\n",
    "\n",
    "What is the core idea behind word embedding? Please, be specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core idea is to map words to high-dimensional vectors that capture their meaning based on the context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 (4 points) \n",
    "\n",
    "What is the primary objective of the Word2Vec model during training?\n",
    "\n",
    "- (a) To assign unique one-hot vectors to each word in the vocabulary.\n",
    "- (b) To maximize the frequency of rare words in the dataset.\n",
    "- (c) To optimize word embeddings such that words occurring in similar contexts have similar vector representations.\n",
    "- (d) To generate dense vectors where all words are equidistant in the embedding space.\n",
    "- (e) All of the above.\n",
    "- (f) None of the above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6 (6 points)\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are often used for sequential data tasks like language modeling and time-series forecasting. Explain how an RNN processes sequential data and describe one limitation of standard RNNs. How can this limitation be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs have the ability to maintain memory of previous inputs through hidden states which helps to capture relationships. One limitation is their difficulty in retaining long-term relationships in data because of vanishing gradients. To address this issue LSTMs are much better at maintaining information over a long period of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7 (4 points)\n",
    "\n",
    "Which of the following components in an LSTM network is responsible for deciding how much of the past information should be retained in the memory cell?\n",
    "\n",
    "- (a) Input gate.\n",
    "- (b) Forget gate. \n",
    "- (c) Output gate.\n",
    "- (d) Hiddent state.\n",
    "- (e) All of the above.\n",
    "- (f) None of the above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8 (4 points)\n",
    "\n",
    "What advantages does FastText offer over Word2Vec? Please, be specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastText is better at handling OOV words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9 (4 points)\n",
    "\n",
    "Which of the following best describes the primary advantage of using Recurrent Neural Networks (RNNs) over traditional feedforward neural networks for sequential data?\n",
    "\n",
    "- (a) RNNs can handle static input data efficiently.\n",
    "- (b) RNNs can process sequential data and capture temporal dependencies by maintaining a hidden state. \n",
    "- (c) RNNs are computationally faster than feedforward networks.\n",
    "- (d) RNNs do not require training over multiple epochs.\n",
    "- (e) All of the above.\n",
    "- (f) None of the above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10\n",
    "\n",
    "Consider the `Tweets.csv` data file. This file contains tweets related to problems with majors US airlines. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \"late flight\" or \"rude service\"). In this exercise, we will focus on building neural networks to classify the sentiment of the tweets.\n",
    "\n",
    "### Exercise 10(a) (3 points)\n",
    "\n",
    "Load `pandas`, `numpy`, and `tensforflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10(b) (3 points)\n",
    "\n",
    "Read the `Tweets.csv` data file, and create a data-frame called `df`. Report the shape of `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Tweets.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10(c) (2 points)\n",
    "\n",
    "Only keep `airline_sentiment` and `text` in `df`. Remove the other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                @VirginAmerica What @dhepburn said.\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing..."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['airline_sentiment', 'text']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10(d) (3 points)\n",
    "\n",
    "Report the frequency table of `airline_sentiment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airline_sentiment\n",
       "negative    62.691257\n",
       "neutral     21.168033\n",
       "positive    16.140710\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['airline_sentiment'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10(e) (3 points)\n",
    "\n",
    "Using the appropriate commands, map `negative -> 0`, `neutral -> 1`, and `positive -> 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   airline_sentiment                                               text\n",
       "0                  1                @VirginAmerica What @dhepburn said.\n",
       "1                  2  @VirginAmerica plus you've added commercials t...\n",
       "2                  1  @VirginAmerica I didn't today... Must mean I n...\n",
       "3                  0  @VirginAmerica it's really aggressive to blast...\n",
       "4                  0  @VirginAmerica and it's a really big bad thing..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['airline_sentiment'] = df['airline_sentiment'].map({'negative': 0, 'neutral': 1, 'positive': 2})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise  10(f) (6 points)\n",
    "\n",
    "Using the appropriate command, remove the tags from each of the texts. For example, the text `@VirginAmerica What @dhepburn said` should be `What said` after removing the tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>What  said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>plus you've added commercials to the experien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I didn't today... Must mean I need to take an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>it's really aggressive to blast obnoxious \"en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>and it's a really big bad thing about it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   airline_sentiment                                               text\n",
       "0                  1                                        What  said.\n",
       "1                  2   plus you've added commercials to the experien...\n",
       "2                  1   I didn't today... Must mean I need to take an...\n",
       "3                  0   it's really aggressive to blast obnoxious \"en...\n",
       "4                  0           and it's a really big bad thing about it"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].str.replace(r'@\\w+', '', regex=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10(g) (4 points)\n",
    "\n",
    "Using the `string` library, remove all the punctuation of the `text` column and store the results in another column called `text_clean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>What  said.</td>\n",
       "      <td>What  said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>plus you've added commercials to the experien...</td>\n",
       "      <td>plus youve added commercials to the experienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I didn't today... Must mean I need to take an...</td>\n",
       "      <td>I didnt today Must mean I need to take anothe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>it's really aggressive to blast obnoxious \"en...</td>\n",
       "      <td>its really aggressive to blast obnoxious ente...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>and it's a really big bad thing about it</td>\n",
       "      <td>and its a really big bad thing about it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   airline_sentiment                                               text  \\\n",
       "0                  1                                        What  said.   \n",
       "1                  2   plus you've added commercials to the experien...   \n",
       "2                  1   I didn't today... Must mean I need to take an...   \n",
       "3                  0   it's really aggressive to blast obnoxious \"en...   \n",
       "4                  0           and it's a really big bad thing about it   \n",
       "\n",
       "                                          text_clean  \n",
       "0                                         What  said  \n",
       "1   plus youve added commercials to the experienc...  \n",
       "2   I didnt today Must mean I need to take anothe...  \n",
       "3   its really aggressive to blast obnoxious ente...  \n",
       "4            and its a really big bad thing about it  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "df['text_clean'] = df['text'].str.replace(f'[{string.punctuation}]','', regex=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10(h) (8 points)\n",
    "\n",
    "Remove stopwords, pronouns, and other words from `text_clean` and store in another column called `'text_clean_1'`. Consider the below list as the stopwords list. Also, convert all the words to lowercase.\n",
    "\n",
    "```\n",
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
    "'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', \n",
    "'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', \n",
    "'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    " 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are',\n",
    " 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing',\n",
    " 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', \n",
    "'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', \n",
    "'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', \n",
    "'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', \n",
    "'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \n",
    "'s', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y',\n",
    " 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", \n",
    "'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn',\n",
    " \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", \n",
    "'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_clean_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>What  said.</td>\n",
       "      <td>What  said</td>\n",
       "      <td>said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>plus you've added commercials to the experien...</td>\n",
       "      <td>plus youve added commercials to the experienc...</td>\n",
       "      <td>plus youve added commercials experience tacky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I didn't today... Must mean I need to take an...</td>\n",
       "      <td>I didnt today Must mean I need to take anothe...</td>\n",
       "      <td>didnt today must mean need take another trip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>it's really aggressive to blast obnoxious \"en...</td>\n",
       "      <td>its really aggressive to blast obnoxious ente...</td>\n",
       "      <td>really aggressive blast obnoxious entertainmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>and it's a really big bad thing about it</td>\n",
       "      <td>and its a really big bad thing about it</td>\n",
       "      <td>really big bad thing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   airline_sentiment                                               text  \\\n",
       "0                  1                                        What  said.   \n",
       "1                  2   plus you've added commercials to the experien...   \n",
       "2                  1   I didn't today... Must mean I need to take an...   \n",
       "3                  0   it's really aggressive to blast obnoxious \"en...   \n",
       "4                  0           and it's a really big bad thing about it   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0                                         What  said   \n",
       "1   plus youve added commercials to the experienc...   \n",
       "2   I didnt today Must mean I need to take anothe...   \n",
       "3   its really aggressive to blast obnoxious ente...   \n",
       "4            and its a really big bad thing about it   \n",
       "\n",
       "                                        text_clean_1  \n",
       "0                                               said  \n",
       "1      plus youve added commercials experience tacky  \n",
       "2       didnt today must mean need take another trip  \n",
       "3  really aggressive blast obnoxious entertainmen...  \n",
       "4                               really big bad thing  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
    "'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', \n",
    "'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', \n",
    "'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    " 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are',\n",
    " 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing',\n",
    " 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', \n",
    "'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', \n",
    "'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', \n",
    "'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', \n",
    "'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \n",
    "'s', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y',\n",
    " 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", \n",
    "'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn',\n",
    " \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", \n",
    "'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "def clean_text(text):\n",
    "    #tokenize text\n",
    "    tokens = text.lower().split()\n",
    "    \n",
    "    #remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['text_clean_1'] = df['text_clean'].apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10(i) (6 points)\n",
    " \n",
    "Based on the results from part 10(h), we see digits in some of the text in `text_clean_1`. Remove all the digits from `text_clean_1` and store the results in another column `text_clean_2` in the `df` data-frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_clean_1</th>\n",
       "      <th>text_clean_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>What  said.</td>\n",
       "      <td>What  said</td>\n",
       "      <td>said</td>\n",
       "      <td>said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>plus you've added commercials to the experien...</td>\n",
       "      <td>plus youve added commercials to the experienc...</td>\n",
       "      <td>plus youve added commercials experience tacky</td>\n",
       "      <td>plus youve added commercials experience tacky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I didn't today... Must mean I need to take an...</td>\n",
       "      <td>I didnt today Must mean I need to take anothe...</td>\n",
       "      <td>didnt today must mean need take another trip</td>\n",
       "      <td>didnt today must mean need take another trip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>it's really aggressive to blast obnoxious \"en...</td>\n",
       "      <td>its really aggressive to blast obnoxious ente...</td>\n",
       "      <td>really aggressive blast obnoxious entertainmen...</td>\n",
       "      <td>really aggressive blast obnoxious entertainmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>and it's a really big bad thing about it</td>\n",
       "      <td>and its a really big bad thing about it</td>\n",
       "      <td>really big bad thing</td>\n",
       "      <td>really big bad thing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   airline_sentiment                                               text  \\\n",
       "0                  1                                        What  said.   \n",
       "1                  2   plus you've added commercials to the experien...   \n",
       "2                  1   I didn't today... Must mean I need to take an...   \n",
       "3                  0   it's really aggressive to blast obnoxious \"en...   \n",
       "4                  0           and it's a really big bad thing about it   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0                                         What  said   \n",
       "1   plus youve added commercials to the experienc...   \n",
       "2   I didnt today Must mean I need to take anothe...   \n",
       "3   its really aggressive to blast obnoxious ente...   \n",
       "4            and its a really big bad thing about it   \n",
       "\n",
       "                                        text_clean_1  \\\n",
       "0                                               said   \n",
       "1      plus youve added commercials experience tacky   \n",
       "2       didnt today must mean need take another trip   \n",
       "3  really aggressive blast obnoxious entertainmen...   \n",
       "4                               really big bad thing   \n",
       "\n",
       "                                        text_clean_2  \n",
       "0                                               said  \n",
       "1      plus youve added commercials experience tacky  \n",
       "2       didnt today must mean need take another trip  \n",
       "3  really aggressive blast obnoxious entertainmen...  \n",
       "4                               really big bad thing  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_clean_2'] = df['text_clean_1'].str.replace(r'\\d+', '', regex=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10(j) (10 points)\n",
    "\n",
    "Use `Word2Vec` from `gensim` library to create embedding for `text_clean_2`. Use the following configuration for `Word2Vec`:\n",
    "\n",
    "```\n",
    "Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "```\n",
    "\n",
    "Store the embedding in a matrix called `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.31753534e-01  5.99199653e-01  1.87974706e-01 -8.99353400e-02\n",
      "   4.00192216e-02 -7.63836086e-01  2.24639386e-01  9.96093392e-01\n",
      "  -5.40306985e-01 -4.59560871e-01 -4.02285606e-02 -9.41854417e-01\n",
      "   7.98774511e-02  4.57536787e-01  9.26087871e-02 -5.16355097e-01\n",
      "   1.57790035e-02 -6.49018168e-01  1.97697252e-01 -9.38001990e-01\n",
      "   3.77108455e-01  5.37733257e-01  3.09603781e-01 -4.03701216e-01\n",
      "  -2.57641017e-01 -7.03191943e-03 -1.52345166e-01 -4.51027423e-01\n",
      "  -3.10240775e-01  9.47487801e-02  5.34448981e-01  3.45998168e-01\n",
      "   3.28075618e-01 -5.65358281e-01 -4.03875768e-01  8.00552964e-01\n",
      "   2.68943101e-01 -4.89334524e-01 -1.58748299e-01 -1.04229879e+00\n",
      "  -1.47593454e-01 -3.66295904e-01 -2.56934136e-01 -1.93005279e-01\n",
      "   4.15544212e-01 -9.27469432e-02 -3.23306024e-01  5.48968278e-02\n",
      "   3.80828917e-01  2.13037938e-01  3.43944103e-01 -6.37330770e-01\n",
      "   1.34028181e-01  3.04786265e-01 -1.22488908e-01  2.90844560e-01\n",
      "   2.70401925e-01 -7.59000927e-02 -7.44798541e-01  6.02678738e-05\n",
      "   7.76037127e-02  2.67095864e-01 -4.09630537e-01 -1.09870039e-01\n",
      "  -6.30587161e-01  5.58999181e-01 -1.57003906e-02  2.19545946e-01\n",
      "  -7.72427797e-01  1.31418300e+00 -3.63198847e-01  2.68257201e-01\n",
      "   6.09737039e-01 -4.29667950e-01  5.36983371e-01  2.07418650e-01\n",
      "  -5.07212095e-02 -2.10348845e-01 -2.09347859e-01  1.69199541e-01\n",
      "  -2.74732858e-01 -2.76581526e-01 -8.35962653e-01  6.95526719e-01\n",
      "  -1.94778293e-01  2.53634065e-01  2.81731002e-02  7.42176533e-01\n",
      "   9.40347135e-01  1.79117158e-01  8.89288783e-01  2.85333306e-01\n",
      "  -3.28295529e-02  2.25207105e-01  1.16856372e+00  3.20610911e-01\n",
      "   1.16154380e-01 -6.09676898e-01  2.56827354e-01 -3.44700038e-01]\n",
      " [-3.25821415e-02  2.46412039e-01  9.57276747e-02 -4.81142364e-02\n",
      "   1.99919716e-02 -3.08725089e-01  1.08540751e-01  4.23877448e-01\n",
      "  -2.49060169e-01 -1.97361156e-01 -1.97011158e-02 -3.96402746e-01\n",
      "   3.35012861e-02  2.06073463e-01  1.89085994e-02 -2.26125062e-01\n",
      "  -1.79664344e-02 -2.57539034e-01  9.90516245e-02 -4.04382467e-01\n",
      "   1.86586276e-01  2.01389015e-01  1.25683174e-01 -1.93519190e-01\n",
      "  -1.17030673e-01 -1.70782637e-02 -5.16844392e-02 -1.88825384e-01\n",
      "  -1.69662908e-01  3.06447987e-02  2.28315398e-01  1.71914801e-01\n",
      "   1.28335416e-01 -2.69135356e-01 -1.37703910e-01  3.32922429e-01\n",
      "   1.02545939e-01 -2.12388560e-01 -8.09041783e-02 -4.55291033e-01\n",
      "  -4.65552807e-02 -1.75409079e-01 -1.32611811e-01 -4.24157381e-02\n",
      "   2.15432540e-01 -5.29412925e-02 -1.30934522e-01  2.76973490e-02\n",
      "   1.81255743e-01  8.42466131e-02  1.48275733e-01 -2.71241635e-01\n",
      "   5.44728450e-02  1.08523749e-01 -5.99236377e-02  1.22043572e-01\n",
      "   8.19708928e-02 -4.26660813e-02 -3.43620569e-01 -3.05642243e-02\n",
      "   3.73036861e-02  1.16722375e-01 -1.78397521e-01 -4.52353694e-02\n",
      "  -2.82683104e-01  2.27454647e-01  5.53640956e-03  1.38213441e-01\n",
      "  -3.22993100e-01  5.92831373e-01 -1.89993128e-01  1.19229086e-01\n",
      "   2.51324445e-01 -1.69968858e-01  2.37899467e-01  1.04057558e-01\n",
      "  -1.36012735e-03 -9.93334949e-02 -7.33903125e-02  4.81953025e-02\n",
      "  -1.42964408e-01 -1.42078742e-01 -3.79960895e-01  3.08309823e-01\n",
      "  -7.85520151e-02  8.86903778e-02  4.25938554e-02  3.30124855e-01\n",
      "   4.20639783e-01  7.64974430e-02  3.57928634e-01  1.25950858e-01\n",
      "  -2.49442607e-02  1.08011730e-01  5.13842404e-01  1.33861095e-01\n",
      "   3.97436991e-02 -2.41564438e-01  1.06705867e-01 -1.64672986e-01]\n",
      " [-7.07483366e-02  6.74924433e-01  2.22844288e-01 -8.33457857e-02\n",
      "   3.00747789e-02 -8.91679823e-01  1.82524458e-01  1.12381864e+00\n",
      "  -5.22332788e-01 -5.15563548e-01 -3.41778994e-02 -9.07449245e-01\n",
      "   5.76634258e-02  4.45152909e-01  1.38897985e-01 -5.07509232e-01\n",
      "   5.17085344e-02 -6.87876642e-01  1.83501974e-01 -1.00039291e+00\n",
      "   4.33614433e-01  5.67642033e-01  2.89637744e-01 -3.71385157e-01\n",
      "  -2.88443208e-01  1.61580853e-02 -1.70376658e-01 -4.33380544e-01\n",
      "  -3.07630718e-01  1.16510831e-01  5.08725584e-01  3.39976132e-01\n",
      "   3.11184138e-01 -6.46224141e-01 -3.58815432e-01  9.19083834e-01\n",
      "   3.49566013e-01 -5.28137445e-01 -2.11447492e-01 -1.11616588e+00\n",
      "  -1.68260977e-01 -3.86119962e-01 -3.00250679e-01 -2.07526878e-01\n",
      "   4.26069975e-01 -7.84579664e-02 -3.28266710e-01  7.62503371e-02\n",
      "   3.88018131e-01  2.37652034e-01  3.67041409e-01 -6.85140252e-01\n",
      "   3.20183262e-02  2.85518050e-01 -1.05691634e-01  2.96068549e-01\n",
      "   3.09206009e-01  5.43016754e-03 -7.87232339e-01  1.94868771e-04\n",
      "   3.60981724e-03  2.65548766e-01 -3.49151880e-01 -7.46893138e-02\n",
      "  -5.88634372e-01  6.22876167e-01 -5.71358763e-03  2.57336944e-01\n",
      "  -8.94993901e-01  1.28947008e+00 -4.00524080e-01  2.19217956e-01\n",
      "   6.04390681e-01 -4.55173194e-01  5.76974094e-01  2.14413121e-01\n",
      "  -4.84737232e-02 -1.86187267e-01 -2.49279708e-01  1.93037257e-01\n",
      "  -2.74215281e-01 -1.97729886e-01 -8.13346922e-01  7.43830681e-01\n",
      "  -2.57067919e-01  2.72647440e-01  9.91167128e-02  7.83599138e-01\n",
      "   8.90031934e-01  2.33832449e-01  8.78271401e-01  2.79003382e-01\n",
      "  -1.78757347e-02  2.32400239e-01  1.24373865e+00  3.60423356e-01\n",
      "   6.39343038e-02 -6.60264432e-01  1.92419380e-01 -3.70375812e-01]\n",
      " [-3.88717875e-02  2.66369641e-01  8.93832296e-02 -3.87324318e-02\n",
      "   2.04919241e-02 -3.44950020e-01  8.75614136e-02  4.45060909e-01\n",
      "  -2.23026723e-01 -2.06418902e-01 -1.29208220e-02 -3.81800801e-01\n",
      "   2.93230899e-02  1.87966138e-01  5.59772961e-02 -2.19446182e-01\n",
      "   1.12576354e-02 -2.77512550e-01  8.04153830e-02 -4.06665087e-01\n",
      "   1.70828670e-01  2.25460261e-01  1.27849221e-01 -1.64795816e-01\n",
      "  -1.21214055e-01 -1.78085605e-03 -6.61699548e-02 -1.89587116e-01\n",
      "  -1.34089395e-01  3.90405618e-02  2.14269832e-01  1.46873832e-01\n",
      "   1.32258132e-01 -2.59639174e-01 -1.49841249e-01  3.48177224e-01\n",
      "   1.23706020e-01 -2.10450739e-01 -7.69812316e-02 -4.48959261e-01\n",
      "  -6.57591149e-02 -1.58413321e-01 -1.22928813e-01 -7.70796612e-02\n",
      "   1.80321664e-01 -3.40850502e-02 -1.36574566e-01  2.56532673e-02\n",
      "   1.60837501e-01  9.33966860e-02  1.53691664e-01 -2.74409980e-01\n",
      "   3.14482935e-02  1.19802855e-01 -4.92862649e-02  1.20165110e-01\n",
      "   1.21221997e-01 -1.12704160e-02 -3.19770277e-01  3.14524770e-03\n",
      "   1.21791232e-02  1.09771088e-01 -1.56996146e-01 -3.76603231e-02\n",
      "  -2.54023015e-01  2.42726281e-01 -8.09682813e-03  1.05902776e-01\n",
      "  -3.47703695e-01  5.43659925e-01 -1.63442075e-01  1.01777792e-01\n",
      "   2.50476539e-01 -1.88089848e-01  2.34388918e-01  8.97204801e-02\n",
      "  -1.71412565e-02 -8.41124952e-02 -9.69066545e-02  6.77354485e-02\n",
      "  -1.14290260e-01 -9.96649191e-02 -3.46879214e-01  3.03407252e-01\n",
      "  -9.43195373e-02  1.11570656e-01  3.22483890e-02  3.22705090e-01\n",
      "   3.83055478e-01  8.79785419e-02  3.61347139e-01  1.15076974e-01\n",
      "  -9.42560472e-03  9.82466787e-02  5.07056475e-01  1.45027995e-01\n",
      "   3.92606482e-02 -2.65789330e-01  9.40489247e-02 -1.49586529e-01]\n",
      " [-8.24175179e-02  5.52344203e-01  1.96748763e-01 -9.29727331e-02\n",
      "   4.77810130e-02 -6.94651961e-01  2.06499502e-01  9.06926990e-01\n",
      "  -4.99120206e-01 -4.26798552e-01 -2.39314735e-02 -8.47516596e-01\n",
      "   7.56253228e-02  4.11970824e-01  8.92893374e-02 -4.83599037e-01\n",
      "   3.72097455e-03 -5.84706545e-01  1.89938337e-01 -8.58801425e-01\n",
      "   3.61473799e-01  4.68573928e-01  2.62592882e-01 -3.88071358e-01\n",
      "  -2.51110584e-01 -1.91800632e-02 -1.33074328e-01 -4.11549330e-01\n",
      "  -3.14231068e-01  7.15625212e-02  4.65481639e-01  3.22058588e-01\n",
      "   2.71938771e-01 -5.50600827e-01 -3.30857158e-01  7.37861574e-01\n",
      "   2.33911246e-01 -4.52860594e-01 -1.61765724e-01 -9.73481655e-01\n",
      "  -1.23809963e-01 -3.39795589e-01 -2.61787981e-01 -1.43890455e-01\n",
      "   4.08606738e-01 -8.34731311e-02 -2.92975605e-01  5.23146167e-02\n",
      "   3.47659767e-01  1.94996715e-01  3.15057129e-01 -5.81950963e-01\n",
      "   1.06186956e-01  2.57405728e-01 -1.20309964e-01  2.62033343e-01\n",
      "   2.30414718e-01 -4.66749668e-02 -6.98576808e-01 -1.76376924e-02\n",
      "   5.79415709e-02  2.40456223e-01 -3.60059559e-01 -9.35640410e-02\n",
      "  -5.72877169e-01  4.93376732e-01 -1.62467901e-02  2.41369486e-01\n",
      "  -7.12096930e-01  1.20168710e+00 -3.57382238e-01  2.50179797e-01\n",
      "   5.49072802e-01 -3.85074675e-01  5.05846500e-01  1.86343625e-01\n",
      "  -3.67765352e-02 -1.91140696e-01 -1.79920152e-01  1.24806590e-01\n",
      "  -2.66937375e-01 -2.56117702e-01 -7.72168219e-01  6.50891602e-01\n",
      "  -1.81676149e-01  2.23930836e-01  5.66568933e-02  6.98079705e-01\n",
      "   8.68576467e-01  1.74710467e-01  7.88072705e-01  2.66517341e-01\n",
      "  -3.68544273e-02  2.15860739e-01  1.08517373e+00  3.04303408e-01\n",
      "   8.99707153e-02 -5.41318655e-01  2.18565851e-01 -3.25046360e-01]]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "tokens = df['text_clean_2'].apply(lambda x: x.split())\n",
    "\n",
    "# train word2vec\n",
    "word2vec_model = Word2Vec(sentences=tokens, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# embed text\n",
    "def get_avg_word2vec(token_list, model, vector_size):\n",
    "    #remove out-of-vocab words\n",
    "    valid_tokens = [word for word in token_list if word in model.wv]\n",
    "    \n",
    "    if not valid_tokens:\n",
    "        return np.zeros(vector_size)\n",
    "    \n",
    "    return np.mean(model.wv[valid_tokens], axis=0)\n",
    "\n",
    "# apply function to get embeddings\n",
    "X = tokens.apply(lambda x: get_avg_word2vec(x, word2vec_model, 100))\n",
    "X = np.vstack(X.values)\n",
    "print(X[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10(k) (10 points)\n",
    "\n",
    "Build a neural network as follows:\n",
    "\n",
    "- Build a neural network as follows:\n",
    "   - `BatchNormalization()` layer.\n",
    "   - `Dense` layer with 16 neurons and `activation=mish`.\n",
    "   - `Dropout(0.01)`\n",
    "   - `Dense` with 3 neuron `activation=softmax`.\n",
    "- Use `optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)`, `loss='categorical_crossentropy'`, and `metrics=['accuracy']`.\n",
    "- Train the network over 5 folds.\n",
    "- Use `epochs=10` and `batch_size=32`. \n",
    "- Report the 5-fold mean accuracy. \n",
    "\n",
    "Use `X` as the inpute and `df[airline_sentiment]` as the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gmgma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, 0.6926229508196722\n",
      "Fold 2, 0.7025273224043715\n",
      "Fold 3, 0.6974043715846995\n",
      "Fold 4, 0.7056010928961749\n",
      "Fold 5, 0.7151639344262295\n",
      "average accuracy 0.7026639344262294\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold as skf\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y = tf.keras.utils.to_categorical(df['airline_sentiment'].astype(int), num_classes=3)\n",
    "\n",
    "md1 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(16, activation='mish', input_shape= (X.shape[1],)),\n",
    "    tf.keras.layers.Dropout(0.01),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "md1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# k fold\n",
    "kfold = skf(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = []\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(kfold.split(X,df['airline_sentiment'])):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    md1.fit(X_train, y_train, epochs = 10, batch_size = 32, verbose=0)\n",
    "    \n",
    "    y_pred = md1.predict(X_test, batch_size=32, verbose=0)\n",
    "    accuracy = accuracy_score(y_pred.argmax(1), y_test.argmax(1))\n",
    "    scores.append(accuracy)\n",
    "    print(f'Fold {i+1}, {accuracy}')\n",
    "    \n",
    "print('average accuracy', np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10(l) (10 points)\n",
    "\n",
    "Build a neural network as follows:\n",
    "\n",
    "- Build a neural network as follows:\n",
    "   - `BatchNormalization()` layer.\n",
    "   - `Dense` layer with 16 neurons and `activation=mish`.\n",
    "   - `Dropout(0.01)`\n",
    "   - `Dense` layer with 16 neurons and `activation=mish`.\n",
    "   - `Dropout(0.01)`\n",
    "   - `Dense` with 3 neuron `activation=softmax`.\n",
    "- Use `optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)`, `loss='categorical_crossentropy'`, and `metrics=['accuracy']`.\n",
    "- Train the network over 5 folds.\n",
    "- Use `epochs=10` and `batch_size=32`. \n",
    "- Report the 5-fold mean accuracy. \n",
    "\n",
    "Use `X` as the inpute and `df[airline_sentiment]` as the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gmgma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, 0.6922814207650273\n",
      "Fold 2, 0.6936475409836066\n",
      "Fold 3, 0.7161885245901639\n",
      "Fold 4, 0.7144808743169399\n",
      "Fold 5, 0.7209699453551912\n",
      "average accuracy 0.7075136612021857\n"
     ]
    }
   ],
   "source": [
    "md2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(16, activation='mish', input_shape= (X.shape[1],)),\n",
    "    tf.keras.layers.Dropout(0.01),\n",
    "    tf.keras.layers.Dense(16, activation='mish'),\n",
    "    tf.keras.layers.Dropout(0.01),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "md2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# k fold\n",
    "kfold = skf(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = []\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(kfold.split(X,df['airline_sentiment'])):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    md2.fit(X_train, y_train, epochs = 10, batch_size = 32, verbose=0)\n",
    "    \n",
    "    y_pred = md2.predict(X_test, batch_size=32, verbose=0)\n",
    "    accuracy = accuracy_score(y_pred.argmax(1), y_test.argmax(1))\n",
    "    scores.append(accuracy)\n",
    "    print(f'Fold {i+1}, {accuracy}')\n",
    "    \n",
    "print('average accuracy', np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10(m) (3 points)\n",
    "\n",
    "Based on your results from parts 10(k) and 10(l), what model woudl use to predict the tweet sentiment? Please, be specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on my results i would used model 2 because it has the higher accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10(n) (15 points)\n",
    "\n",
    "Build a neural network that outperforms the best model from parts 10(k) and 10(l)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gmgma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, 0.6956967213114754\n",
      "Fold 2, 0.6956967213114754\n",
      "Fold 3, 0.7257513661202186\n",
      "Fold 4, 0.7172131147540983\n",
      "Fold 5, 0.7161885245901639\n",
      "average accuracy 0.7101092896174863\n"
     ]
    }
   ],
   "source": [
    "md3 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(32, activation='mish', input_shape= (X.shape[1],)),\n",
    "    tf.keras.layers.Dropout(0.01),\n",
    "    tf.keras.layers.Dense(32, activation='mish'),\n",
    "    tf.keras.layers.Dropout(0.01),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "md3.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# k fold\n",
    "kfold = skf(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = []\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(kfold.split(X,df['airline_sentiment'])):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    md3.fit(X_train, y_train, epochs = 10, batch_size = 32, verbose=0)\n",
    "    \n",
    "    y_pred = md3.predict(X_test, batch_size=32, verbose=0)\n",
    "    accuracy = accuracy_score(y_pred.argmax(1), y_test.argmax(1))\n",
    "    scores.append(accuracy)\n",
    "    print(f'Fold {i+1}, {accuracy}')\n",
    "    \n",
    "print('average accuracy', np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got better results using 32 neurons for both layers rather than 16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
