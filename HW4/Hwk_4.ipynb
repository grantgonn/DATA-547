{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "In this exercise, we will practice how to use deep neural networks on image data. We will be using the popular [Fashion mnist](https://www.tensorflow.org/datasets/catalog/fashion_mnist), which consists of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image associated with a label from 10 classes. Our goal is to build a neural network model to predict the label of a given image. We will achieve this in the following exercises.\n",
    "\n",
    "### Exercise 1(a) (2 points)\n",
    "\n",
    "Load the below libraries.\n",
    "\n",
    "```\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1(b) (2 points)\n",
    "\n",
    "Load the `fashion_mnist` data as follows:\n",
    "\n",
    "```\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1(c) (2 points)\n",
    "\n",
    "Report the shape of the `train` and `test` data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (60000, 28, 28)\n",
      "Test data shape: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print('Train data shape:', x_train.shape)\n",
    "print('Test data shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1(d) (7 points)\n",
    "\n",
    "Write Python code to randomly select and visualize 6 images from the `train` data set. Notice that image label are the following:\n",
    "\n",
    "```\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x2000 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_images = 6\n",
    "\n",
    "fig, axes = plt.subplots(1, num_images, figsize=(20,20))\n",
    "for i in range(num_images):\n",
    "    sample = np.random.randint(0, x_train.shape[0])\n",
    "    axes[i].imshow(x_train[sample], cmap='gray')\n",
    "    axes[i].set_title(y_train[sample])\n",
    "    axes[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1(e) (12 points)\n",
    "\n",
    "Build a deep neural network as follows:\n",
    "\n",
    "- Reshape the input features. Use 784.\n",
    "- Standardize the input features. Divide the input values by 255.\n",
    "- Change the digit labels to 0-1 encoding.\n",
    "- Build a deep neural network with 2 layers with 64 neurons. Use `relu` as the activation function in the hidden layers and `softmax` as the activation function in the output.\n",
    "- Compile the network with the following:\n",
    "    - `optimizer='adam'`\n",
    "    - `loss='categorical_crossentropy'`\n",
    "    - `metrics=['accuracy']`\n",
    "- Train the deep neural network with `epochs=50`, `batch_size=64`, and `validation_split=0.1`.\n",
    "- Evaluate the model in the `test` data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7335 - loss: 0.7536 - val_accuracy: 0.8438 - val_loss: 0.4174\n",
      "Epoch 2/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8547 - loss: 0.4032 - val_accuracy: 0.8575 - val_loss: 0.3888\n",
      "Epoch 3/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8709 - loss: 0.3592 - val_accuracy: 0.8745 - val_loss: 0.3471\n",
      "Epoch 4/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8744 - loss: 0.3392 - val_accuracy: 0.8677 - val_loss: 0.3534\n",
      "Epoch 5/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8876 - loss: 0.3068 - val_accuracy: 0.8708 - val_loss: 0.3417\n",
      "Epoch 6/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8894 - loss: 0.3060 - val_accuracy: 0.8710 - val_loss: 0.3560\n",
      "Epoch 7/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8930 - loss: 0.2881 - val_accuracy: 0.8780 - val_loss: 0.3344\n",
      "Epoch 8/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8960 - loss: 0.2828 - val_accuracy: 0.8822 - val_loss: 0.3341\n",
      "Epoch 9/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8994 - loss: 0.2682 - val_accuracy: 0.8842 - val_loss: 0.3249\n",
      "Epoch 10/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9039 - loss: 0.2582 - val_accuracy: 0.8877 - val_loss: 0.3168\n",
      "Epoch 11/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9090 - loss: 0.2459 - val_accuracy: 0.8825 - val_loss: 0.3294\n",
      "Epoch 12/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9070 - loss: 0.2496 - val_accuracy: 0.8873 - val_loss: 0.3207\n",
      "Epoch 13/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9138 - loss: 0.2351 - val_accuracy: 0.8833 - val_loss: 0.3411\n",
      "Epoch 14/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9171 - loss: 0.2216 - val_accuracy: 0.8860 - val_loss: 0.3242\n",
      "Epoch 15/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9180 - loss: 0.2213 - val_accuracy: 0.8862 - val_loss: 0.3197\n",
      "Epoch 16/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9194 - loss: 0.2183 - val_accuracy: 0.8868 - val_loss: 0.3373\n",
      "Epoch 17/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9203 - loss: 0.2138 - val_accuracy: 0.8923 - val_loss: 0.3195\n",
      "Epoch 18/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9248 - loss: 0.2039 - val_accuracy: 0.8893 - val_loss: 0.3229\n",
      "Epoch 19/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9248 - loss: 0.2014 - val_accuracy: 0.8905 - val_loss: 0.3394\n",
      "Epoch 20/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9260 - loss: 0.1990 - val_accuracy: 0.8907 - val_loss: 0.3304\n",
      "Epoch 21/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9273 - loss: 0.1933 - val_accuracy: 0.8912 - val_loss: 0.3406\n",
      "Epoch 22/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9276 - loss: 0.1951 - val_accuracy: 0.8897 - val_loss: 0.3484\n",
      "Epoch 23/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9303 - loss: 0.1885 - val_accuracy: 0.8883 - val_loss: 0.3588\n",
      "Epoch 24/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9341 - loss: 0.1822 - val_accuracy: 0.8905 - val_loss: 0.3382\n",
      "Epoch 25/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9352 - loss: 0.1724 - val_accuracy: 0.8853 - val_loss: 0.3638\n",
      "Epoch 26/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9343 - loss: 0.1754 - val_accuracy: 0.8822 - val_loss: 0.3870\n",
      "Epoch 27/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9366 - loss: 0.1720 - val_accuracy: 0.8897 - val_loss: 0.3634\n",
      "Epoch 28/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9377 - loss: 0.1712 - val_accuracy: 0.8873 - val_loss: 0.3901\n",
      "Epoch 29/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9362 - loss: 0.1708 - val_accuracy: 0.8907 - val_loss: 0.3771\n",
      "Epoch 30/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9397 - loss: 0.1624 - val_accuracy: 0.8888 - val_loss: 0.3806\n",
      "Epoch 31/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9422 - loss: 0.1592 - val_accuracy: 0.8868 - val_loss: 0.3790\n",
      "Epoch 32/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9406 - loss: 0.1570 - val_accuracy: 0.8903 - val_loss: 0.3754\n",
      "Epoch 33/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9420 - loss: 0.1545 - val_accuracy: 0.8928 - val_loss: 0.3775\n",
      "Epoch 34/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9456 - loss: 0.1466 - val_accuracy: 0.8907 - val_loss: 0.3931\n",
      "Epoch 35/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9428 - loss: 0.1519 - val_accuracy: 0.8880 - val_loss: 0.4117\n",
      "Epoch 36/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9467 - loss: 0.1464 - val_accuracy: 0.8870 - val_loss: 0.3994\n",
      "Epoch 37/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9468 - loss: 0.1453 - val_accuracy: 0.8907 - val_loss: 0.3943\n",
      "Epoch 38/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9481 - loss: 0.1407 - val_accuracy: 0.8920 - val_loss: 0.4004\n",
      "Epoch 39/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9494 - loss: 0.1382 - val_accuracy: 0.8843 - val_loss: 0.4302\n",
      "Epoch 40/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9468 - loss: 0.1417 - val_accuracy: 0.8895 - val_loss: 0.4037\n",
      "Epoch 41/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9529 - loss: 0.1303 - val_accuracy: 0.8885 - val_loss: 0.4318\n",
      "Epoch 42/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9508 - loss: 0.1324 - val_accuracy: 0.8863 - val_loss: 0.4249\n",
      "Epoch 43/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9489 - loss: 0.1352 - val_accuracy: 0.8872 - val_loss: 0.4232\n",
      "Epoch 44/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9528 - loss: 0.1275 - val_accuracy: 0.8908 - val_loss: 0.4195\n",
      "Epoch 45/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9550 - loss: 0.1215 - val_accuracy: 0.8777 - val_loss: 0.4830\n",
      "Epoch 46/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9536 - loss: 0.1253 - val_accuracy: 0.8890 - val_loss: 0.4292\n",
      "Epoch 47/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9542 - loss: 0.1220 - val_accuracy: 0.8855 - val_loss: 0.4502\n",
      "Epoch 48/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9557 - loss: 0.1203 - val_accuracy: 0.8933 - val_loss: 0.4495\n",
      "Epoch 49/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9592 - loss: 0.1114 - val_accuracy: 0.8857 - val_loss: 0.4782\n",
      "Epoch 50/50\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9575 - loss: 0.1177 - val_accuracy: 0.8838 - val_loss: 0.4738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x19904c5ccd0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "md1 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(784,)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "md1.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "md1.fit(x_train, y_train, epochs = 50, batch_size = 64, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1(f) (10 points)\n",
    "\n",
    "Build a deep neural network as follows:\n",
    "\n",
    "- Build a deep neural network with 2 layers with 128 neurons. Use `relu` as the activation function in the hidden layers and `softmax` as the activation function in the output.\n",
    "- Compile the network with the following:\n",
    "    - `optimizer='adam'`\n",
    "    - `loss='categorical_crossentropy'`\n",
    "    - `metrics=['accuracy']`\n",
    "- Train the deep neural network with `epochs=50`, `batch_size=128`, and `validation_split=0.1`.\n",
    "- Evaluate the model in the `test` data set. \n",
    "\n",
    "Notice that there is no need to reshape the data; you can go ahead and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7462 - loss: 0.7534 - val_accuracy: 0.8495 - val_loss: 0.4164\n",
      "Epoch 2/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8566 - loss: 0.4045 - val_accuracy: 0.8675 - val_loss: 0.3656\n",
      "Epoch 3/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8735 - loss: 0.3535 - val_accuracy: 0.8683 - val_loss: 0.3543\n",
      "Epoch 4/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8827 - loss: 0.3239 - val_accuracy: 0.8782 - val_loss: 0.3426\n",
      "Epoch 5/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8899 - loss: 0.3021 - val_accuracy: 0.8687 - val_loss: 0.3603\n",
      "Epoch 6/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8928 - loss: 0.2887 - val_accuracy: 0.8757 - val_loss: 0.3403\n",
      "Epoch 7/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8998 - loss: 0.2712 - val_accuracy: 0.8830 - val_loss: 0.3168\n",
      "Epoch 8/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9043 - loss: 0.2583 - val_accuracy: 0.8868 - val_loss: 0.3209\n",
      "Epoch 9/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9080 - loss: 0.2473 - val_accuracy: 0.8818 - val_loss: 0.3231\n",
      "Epoch 10/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9124 - loss: 0.2363 - val_accuracy: 0.8858 - val_loss: 0.3202\n",
      "Epoch 11/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9158 - loss: 0.2284 - val_accuracy: 0.8802 - val_loss: 0.3309\n",
      "Epoch 12/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9147 - loss: 0.2244 - val_accuracy: 0.8863 - val_loss: 0.3242\n",
      "Epoch 13/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9203 - loss: 0.2139 - val_accuracy: 0.8877 - val_loss: 0.3146\n",
      "Epoch 14/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9229 - loss: 0.2050 - val_accuracy: 0.8885 - val_loss: 0.3433\n",
      "Epoch 15/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9240 - loss: 0.2070 - val_accuracy: 0.8907 - val_loss: 0.3293\n",
      "Epoch 16/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9286 - loss: 0.1916 - val_accuracy: 0.8893 - val_loss: 0.3174\n",
      "Epoch 17/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9292 - loss: 0.1866 - val_accuracy: 0.8800 - val_loss: 0.3604\n",
      "Epoch 18/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9333 - loss: 0.1793 - val_accuracy: 0.8895 - val_loss: 0.3245\n",
      "Epoch 19/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9327 - loss: 0.1762 - val_accuracy: 0.8858 - val_loss: 0.3540\n",
      "Epoch 20/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9372 - loss: 0.1664 - val_accuracy: 0.8932 - val_loss: 0.3326\n",
      "Epoch 21/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9375 - loss: 0.1658 - val_accuracy: 0.8875 - val_loss: 0.3627\n",
      "Epoch 22/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9377 - loss: 0.1623 - val_accuracy: 0.8917 - val_loss: 0.3541\n",
      "Epoch 23/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9425 - loss: 0.1531 - val_accuracy: 0.8937 - val_loss: 0.3545\n",
      "Epoch 24/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9420 - loss: 0.1503 - val_accuracy: 0.8932 - val_loss: 0.3685\n",
      "Epoch 25/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9423 - loss: 0.1531 - val_accuracy: 0.8950 - val_loss: 0.3498\n",
      "Epoch 26/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9501 - loss: 0.1341 - val_accuracy: 0.8932 - val_loss: 0.3701\n",
      "Epoch 27/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9475 - loss: 0.1396 - val_accuracy: 0.8945 - val_loss: 0.3742\n",
      "Epoch 28/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9519 - loss: 0.1259 - val_accuracy: 0.8918 - val_loss: 0.3703\n",
      "Epoch 29/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9514 - loss: 0.1299 - val_accuracy: 0.8887 - val_loss: 0.3881\n",
      "Epoch 30/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9534 - loss: 0.1226 - val_accuracy: 0.8928 - val_loss: 0.3990\n",
      "Epoch 31/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9547 - loss: 0.1213 - val_accuracy: 0.8950 - val_loss: 0.3814\n",
      "Epoch 32/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9562 - loss: 0.1171 - val_accuracy: 0.8943 - val_loss: 0.4067\n",
      "Epoch 33/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9526 - loss: 0.1208 - val_accuracy: 0.8787 - val_loss: 0.4500\n",
      "Epoch 34/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9572 - loss: 0.1117 - val_accuracy: 0.8935 - val_loss: 0.3997\n",
      "Epoch 35/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9598 - loss: 0.1066 - val_accuracy: 0.8922 - val_loss: 0.4410\n",
      "Epoch 36/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9610 - loss: 0.1009 - val_accuracy: 0.8933 - val_loss: 0.4257\n",
      "Epoch 37/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9621 - loss: 0.0982 - val_accuracy: 0.8943 - val_loss: 0.4184\n",
      "Epoch 38/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9628 - loss: 0.0969 - val_accuracy: 0.8947 - val_loss: 0.4290\n",
      "Epoch 39/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9651 - loss: 0.0945 - val_accuracy: 0.8962 - val_loss: 0.4505\n",
      "Epoch 40/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9651 - loss: 0.0948 - val_accuracy: 0.8970 - val_loss: 0.4343\n",
      "Epoch 41/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.0890 - val_accuracy: 0.8910 - val_loss: 0.4614\n",
      "Epoch 42/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9668 - loss: 0.0843 - val_accuracy: 0.8963 - val_loss: 0.4836\n",
      "Epoch 43/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9692 - loss: 0.0823 - val_accuracy: 0.8940 - val_loss: 0.4536\n",
      "Epoch 44/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9687 - loss: 0.0834 - val_accuracy: 0.8942 - val_loss: 0.4931\n",
      "Epoch 45/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9672 - loss: 0.0849 - val_accuracy: 0.8913 - val_loss: 0.4916\n",
      "Epoch 46/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9704 - loss: 0.0810 - val_accuracy: 0.8937 - val_loss: 0.4949\n",
      "Epoch 47/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9712 - loss: 0.0791 - val_accuracy: 0.8927 - val_loss: 0.5029\n",
      "Epoch 48/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9746 - loss: 0.0686 - val_accuracy: 0.8933 - val_loss: 0.5012\n",
      "Epoch 49/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9722 - loss: 0.0731 - val_accuracy: 0.8972 - val_loss: 0.4949\n",
      "Epoch 50/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9718 - loss: 0.0738 - val_accuracy: 0.8920 - val_loss: 0.5323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x19904f408e0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "md2.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "md2.fit(x_train, y_train, epochs = 50, batch_size = 128, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1(g) (10 points)\n",
    "\n",
    "Build a deep neural network as follows:\n",
    "\n",
    "- Build a deep neural network with 2 layers with 256 neurons. Use `relu` as the activation function in the hidden layers and `softmax` as the activation function in the output.\n",
    "- Compile the network with the following:\n",
    "    - `optimizer='adam'`\n",
    "    - `loss='categorical_crossentropy'`\n",
    "    - `metrics=['accuracy']`\n",
    "- Train the deep neural network with `epochs=50`, `batch_size=256`, and `validation_split=0.1`.\n",
    "- Evaluate the model in the `test` data set. \n",
    "\n",
    "Notice that there is no need to reshape the data; you can go ahead and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7325 - loss: 0.7780 - val_accuracy: 0.8503 - val_loss: 0.4134\n",
      "Epoch 2/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8589 - loss: 0.3969 - val_accuracy: 0.8665 - val_loss: 0.3650\n",
      "Epoch 3/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8757 - loss: 0.3432 - val_accuracy: 0.8742 - val_loss: 0.3541\n",
      "Epoch 4/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8853 - loss: 0.3156 - val_accuracy: 0.8803 - val_loss: 0.3260\n",
      "Epoch 5/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8925 - loss: 0.2917 - val_accuracy: 0.8797 - val_loss: 0.3286\n",
      "Epoch 6/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8976 - loss: 0.2740 - val_accuracy: 0.8807 - val_loss: 0.3360\n",
      "Epoch 7/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9044 - loss: 0.2597 - val_accuracy: 0.8780 - val_loss: 0.3361\n",
      "Epoch 8/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9069 - loss: 0.2511 - val_accuracy: 0.8827 - val_loss: 0.3291\n",
      "Epoch 9/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9130 - loss: 0.2352 - val_accuracy: 0.8895 - val_loss: 0.3035\n",
      "Epoch 10/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9139 - loss: 0.2274 - val_accuracy: 0.8902 - val_loss: 0.3102\n",
      "Epoch 11/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9207 - loss: 0.2153 - val_accuracy: 0.8910 - val_loss: 0.3054\n",
      "Epoch 12/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9218 - loss: 0.2094 - val_accuracy: 0.8850 - val_loss: 0.3349\n",
      "Epoch 13/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9241 - loss: 0.2013 - val_accuracy: 0.8935 - val_loss: 0.3187\n",
      "Epoch 14/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9286 - loss: 0.1924 - val_accuracy: 0.8890 - val_loss: 0.3202\n",
      "Epoch 15/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9298 - loss: 0.1899 - val_accuracy: 0.8893 - val_loss: 0.3162\n",
      "Epoch 16/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9311 - loss: 0.1839 - val_accuracy: 0.8903 - val_loss: 0.3262\n",
      "Epoch 17/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9385 - loss: 0.1672 - val_accuracy: 0.8937 - val_loss: 0.3087\n",
      "Epoch 18/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9375 - loss: 0.1666 - val_accuracy: 0.8888 - val_loss: 0.3294\n",
      "Epoch 19/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9400 - loss: 0.1612 - val_accuracy: 0.8932 - val_loss: 0.3496\n",
      "Epoch 20/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9427 - loss: 0.1567 - val_accuracy: 0.8898 - val_loss: 0.3354\n",
      "Epoch 21/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9466 - loss: 0.1467 - val_accuracy: 0.8925 - val_loss: 0.3436\n",
      "Epoch 22/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9444 - loss: 0.1472 - val_accuracy: 0.8923 - val_loss: 0.3382\n",
      "Epoch 23/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9447 - loss: 0.1437 - val_accuracy: 0.8972 - val_loss: 0.3374\n",
      "Epoch 24/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9519 - loss: 0.1286 - val_accuracy: 0.8943 - val_loss: 0.3309\n",
      "Epoch 25/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9507 - loss: 0.1314 - val_accuracy: 0.8968 - val_loss: 0.3411\n",
      "Epoch 26/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9545 - loss: 0.1215 - val_accuracy: 0.8975 - val_loss: 0.3396\n",
      "Epoch 27/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9556 - loss: 0.1215 - val_accuracy: 0.8952 - val_loss: 0.3595\n",
      "Epoch 28/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9557 - loss: 0.1158 - val_accuracy: 0.8908 - val_loss: 0.3865\n",
      "Epoch 29/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9575 - loss: 0.1136 - val_accuracy: 0.8913 - val_loss: 0.3829\n",
      "Epoch 30/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9625 - loss: 0.1048 - val_accuracy: 0.8952 - val_loss: 0.4003\n",
      "Epoch 31/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9608 - loss: 0.1045 - val_accuracy: 0.8897 - val_loss: 0.4007\n",
      "Epoch 32/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9642 - loss: 0.0974 - val_accuracy: 0.8988 - val_loss: 0.3988\n",
      "Epoch 33/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9630 - loss: 0.0982 - val_accuracy: 0.8987 - val_loss: 0.3847\n",
      "Epoch 34/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9665 - loss: 0.0904 - val_accuracy: 0.8913 - val_loss: 0.4171\n",
      "Epoch 35/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9661 - loss: 0.0889 - val_accuracy: 0.8950 - val_loss: 0.4171\n",
      "Epoch 36/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9657 - loss: 0.0914 - val_accuracy: 0.8945 - val_loss: 0.4431\n",
      "Epoch 37/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9698 - loss: 0.0813 - val_accuracy: 0.9007 - val_loss: 0.4214\n",
      "Epoch 38/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9723 - loss: 0.0758 - val_accuracy: 0.8997 - val_loss: 0.4173\n",
      "Epoch 39/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9714 - loss: 0.0754 - val_accuracy: 0.8932 - val_loss: 0.4324\n",
      "Epoch 40/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9713 - loss: 0.0783 - val_accuracy: 0.8953 - val_loss: 0.4158\n",
      "Epoch 41/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9721 - loss: 0.0771 - val_accuracy: 0.8935 - val_loss: 0.4522\n",
      "Epoch 42/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9739 - loss: 0.0708 - val_accuracy: 0.8990 - val_loss: 0.4174\n",
      "Epoch 43/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9758 - loss: 0.0646 - val_accuracy: 0.8962 - val_loss: 0.4447\n",
      "Epoch 44/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9757 - loss: 0.0638 - val_accuracy: 0.8938 - val_loss: 0.4565\n",
      "Epoch 45/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9759 - loss: 0.0631 - val_accuracy: 0.8943 - val_loss: 0.4680\n",
      "Epoch 46/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9713 - loss: 0.0745 - val_accuracy: 0.8942 - val_loss: 0.4820\n",
      "Epoch 47/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9800 - loss: 0.0556 - val_accuracy: 0.8978 - val_loss: 0.4686\n",
      "Epoch 48/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9811 - loss: 0.0522 - val_accuracy: 0.8883 - val_loss: 0.5159\n",
      "Epoch 49/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9761 - loss: 0.0675 - val_accuracy: 0.8930 - val_loss: 0.4915\n",
      "Epoch 50/50\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9819 - loss: 0.0505 - val_accuracy: 0.8938 - val_loss: 0.5175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x199051da8c0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md3 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation='relu', input_shape=(784,)),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "md3.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "md3.fit(x_train, y_train, epochs = 50, batch_size = 256, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8749 - loss: 0.4880\n",
      "0.8779000043869019\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8905 - loss: 0.5364\n",
      "0.8896999955177307\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8923 - loss: 0.5611\n",
      "0.892300009727478\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = md1.evaluate(x_test, y_test)\n",
    "print(test_acc)\n",
    "test_loss, test_acc = md2.evaluate(x_test, y_test)\n",
    "print(test_acc)\n",
    "test_loss, test_acc = md3.evaluate(x_test, y_test)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1(h) (3 points)\n",
    "\n",
    "Based on the results from parts 1(e)-1(g), which model would you use to predict the label of the `fashion_mnist` data set? Please be specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results I would use model 3 because it has the highest accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "In this exercise, we still use the `fashion_mnist` data set. However, we now will train and evaluated convolutional neural network models.\n",
    "\n",
    "### Exercise 2(a) (2 points)\n",
    "\n",
    "Reload the `fashion_mnist` data as follows:\n",
    "\n",
    "```\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execise 2(b) (12 points)\n",
    "\n",
    "Build a CNN model as follows:\n",
    "\n",
    "- Change the digit labels to 0-1 encoding.\n",
    "- The CNN model should have the following layers in the given order:\n",
    "    - `Conv2D` with 32 filters, `kernel_size=(3,3)` and `activation=relu`\n",
    "    - `MaxPooling2D` with `pool_size=(2,2)`\n",
    "    - `Conv2D` with 32 filters, `kernel_size=(3,3)` and `activation=relu`\n",
    "    - `MaxPooling2D` with `pool_size=(2,2)`\n",
    "    - `Flatten`\n",
    "    - `Dense` with 128 neurons and `activation=relu`\n",
    "    - `Dense` with 10 neurons and `activation=softmax`\n",
    "- Compile the network with the following:\n",
    "    - `optimizer='adam'`\n",
    "    - `loss='categorical_crossentropy'`\n",
    "    - `metrics=['accuracy']`\n",
    "- Train the deep neural network with `epochs=50`, `batch_size=128`, and `validation_split=0.1`.\n",
    "- Evaluate the model on the `test` data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.6954 - loss: 2.5276 - val_accuracy: 0.8508 - val_loss: 0.4236\n",
      "Epoch 2/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.8525 - loss: 0.3988 - val_accuracy: 0.8598 - val_loss: 0.3806\n",
      "Epoch 3/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.8757 - loss: 0.3352 - val_accuracy: 0.8592 - val_loss: 0.3809\n",
      "Epoch 4/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.8857 - loss: 0.3066 - val_accuracy: 0.8697 - val_loss: 0.3650\n",
      "Epoch 5/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.8960 - loss: 0.2748 - val_accuracy: 0.8790 - val_loss: 0.3380\n",
      "Epoch 6/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9086 - loss: 0.2462 - val_accuracy: 0.8782 - val_loss: 0.3448\n",
      "Epoch 7/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9102 - loss: 0.2376 - val_accuracy: 0.8812 - val_loss: 0.3387\n",
      "Epoch 8/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9169 - loss: 0.2212 - val_accuracy: 0.8872 - val_loss: 0.3487\n",
      "Epoch 9/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9241 - loss: 0.1999 - val_accuracy: 0.8873 - val_loss: 0.3403\n",
      "Epoch 10/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9259 - loss: 0.1952 - val_accuracy: 0.8860 - val_loss: 0.3421\n",
      "Epoch 11/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9300 - loss: 0.1839 - val_accuracy: 0.8918 - val_loss: 0.3298\n",
      "Epoch 12/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9383 - loss: 0.1655 - val_accuracy: 0.8772 - val_loss: 0.3898\n",
      "Epoch 13/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9411 - loss: 0.1560 - val_accuracy: 0.8827 - val_loss: 0.3823\n",
      "Epoch 14/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9402 - loss: 0.1580 - val_accuracy: 0.8880 - val_loss: 0.3730\n",
      "Epoch 15/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9481 - loss: 0.1369 - val_accuracy: 0.8827 - val_loss: 0.3863\n",
      "Epoch 16/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9510 - loss: 0.1293 - val_accuracy: 0.8882 - val_loss: 0.3892\n",
      "Epoch 17/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9507 - loss: 0.1289 - val_accuracy: 0.8930 - val_loss: 0.3901\n",
      "Epoch 18/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9533 - loss: 0.1222 - val_accuracy: 0.8898 - val_loss: 0.4112\n",
      "Epoch 19/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9577 - loss: 0.1129 - val_accuracy: 0.8868 - val_loss: 0.4167\n",
      "Epoch 20/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9587 - loss: 0.1095 - val_accuracy: 0.8893 - val_loss: 0.4704\n",
      "Epoch 21/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9604 - loss: 0.1036 - val_accuracy: 0.8862 - val_loss: 0.4743\n",
      "Epoch 22/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9646 - loss: 0.0946 - val_accuracy: 0.8897 - val_loss: 0.4681\n",
      "Epoch 23/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9634 - loss: 0.0975 - val_accuracy: 0.8907 - val_loss: 0.5096\n",
      "Epoch 24/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9616 - loss: 0.0990 - val_accuracy: 0.8830 - val_loss: 0.5464\n",
      "Epoch 25/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9679 - loss: 0.0860 - val_accuracy: 0.8893 - val_loss: 0.5175\n",
      "Epoch 26/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9706 - loss: 0.0786 - val_accuracy: 0.8823 - val_loss: 0.5535\n",
      "Epoch 27/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9708 - loss: 0.0773 - val_accuracy: 0.8900 - val_loss: 0.5408\n",
      "Epoch 28/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9720 - loss: 0.0746 - val_accuracy: 0.8910 - val_loss: 0.5801\n",
      "Epoch 29/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9736 - loss: 0.0693 - val_accuracy: 0.8853 - val_loss: 0.6149\n",
      "Epoch 30/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9737 - loss: 0.0687 - val_accuracy: 0.8877 - val_loss: 0.6573\n",
      "Epoch 31/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9748 - loss: 0.0687 - val_accuracy: 0.8887 - val_loss: 0.6468\n",
      "Epoch 32/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9765 - loss: 0.0631 - val_accuracy: 0.8870 - val_loss: 0.6572\n",
      "Epoch 33/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9756 - loss: 0.0682 - val_accuracy: 0.8898 - val_loss: 0.6686\n",
      "Epoch 34/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9788 - loss: 0.0572 - val_accuracy: 0.8863 - val_loss: 0.7204\n",
      "Epoch 35/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9777 - loss: 0.0606 - val_accuracy: 0.8903 - val_loss: 0.6978\n",
      "Epoch 36/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9796 - loss: 0.0588 - val_accuracy: 0.8953 - val_loss: 0.6939\n",
      "Epoch 37/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9819 - loss: 0.0516 - val_accuracy: 0.8920 - val_loss: 0.7929\n",
      "Epoch 38/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9829 - loss: 0.0466 - val_accuracy: 0.8878 - val_loss: 0.7718\n",
      "Epoch 39/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9810 - loss: 0.0540 - val_accuracy: 0.8843 - val_loss: 0.7327\n",
      "Epoch 40/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9812 - loss: 0.0541 - val_accuracy: 0.8828 - val_loss: 0.7675\n",
      "Epoch 41/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9832 - loss: 0.0454 - val_accuracy: 0.8868 - val_loss: 0.8095\n",
      "Epoch 42/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9823 - loss: 0.0505 - val_accuracy: 0.8880 - val_loss: 0.8169\n",
      "Epoch 43/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9865 - loss: 0.0381 - val_accuracy: 0.8892 - val_loss: 0.8502\n",
      "Epoch 44/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9816 - loss: 0.0534 - val_accuracy: 0.8848 - val_loss: 0.8563\n",
      "Epoch 45/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9829 - loss: 0.0512 - val_accuracy: 0.8865 - val_loss: 0.9041\n",
      "Epoch 46/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9849 - loss: 0.0428 - val_accuracy: 0.8923 - val_loss: 0.9113\n",
      "Epoch 47/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9852 - loss: 0.0405 - val_accuracy: 0.8900 - val_loss: 0.8580\n",
      "Epoch 48/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9824 - loss: 0.0499 - val_accuracy: 0.8892 - val_loss: 0.9474\n",
      "Epoch 49/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.9869 - loss: 0.0398 - val_accuracy: 0.8870 - val_loss: 0.9634\n",
      "Epoch 50/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.9880 - loss: 0.0332 - val_accuracy: 0.8873 - val_loss: 0.9399\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x19903d04d00>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "md1 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size = (2,2)),\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=(3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size = (2,2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "md1.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "md1.fit(x_train, y_train, epochs = 50, batch_size = 128, validation_split = 0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execise 2(c) (10 points)\n",
    "\n",
    "Build a CNN model as follows:\n",
    "\n",
    "- The CNN model should have the following layers in the given order:\n",
    "    - `Conv2D` with 64 filters, `kernel_size=(3,3)` and `activation=relu`\n",
    "    - `MaxPooling2D` with `pool_size=(2,2)`\n",
    "    - `Conv2D` with 64 filters, `kernel_size=(3,3)` and `activation=relu`\n",
    "    - `MaxPooling2D` with `pool_size=(2,2)`\n",
    "    - `Flatten`\n",
    "    - `Dense` with 128 neurons and `activation=relu`\n",
    "    - `Dense` with 10 neurons and `activation=softmax`\n",
    "- Compile the network with the following:\n",
    "    - `optimizer='adam'`\n",
    "    - `loss='categorical_crossentropy'`\n",
    "    - `metrics=['accuracy']`\n",
    "- Train the deep neural network with `epochs=50`, `batch_size=128`, and `validation_split=0.1`.\n",
    "- Evaluate the model on the `test` data set.\n",
    "\n",
    "Notice that there is no need to 0-1 encode the target labels; you can go ahead and train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 26ms/step - accuracy: 0.6955 - loss: 2.6426 - val_accuracy: 0.8613 - val_loss: 0.3920\n",
      "Epoch 2/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 25ms/step - accuracy: 0.8745 - loss: 0.3434 - val_accuracy: 0.8682 - val_loss: 0.3560\n",
      "Epoch 3/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.8914 - loss: 0.2928 - val_accuracy: 0.8788 - val_loss: 0.3238\n",
      "Epoch 4/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9007 - loss: 0.2649 - val_accuracy: 0.8907 - val_loss: 0.3010\n",
      "Epoch 5/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9156 - loss: 0.2256 - val_accuracy: 0.8810 - val_loss: 0.3120\n",
      "Epoch 6/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9186 - loss: 0.2152 - val_accuracy: 0.8895 - val_loss: 0.3181\n",
      "Epoch 7/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9248 - loss: 0.1988 - val_accuracy: 0.8962 - val_loss: 0.2996\n",
      "Epoch 8/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.9324 - loss: 0.1849 - val_accuracy: 0.8907 - val_loss: 0.3215\n",
      "Epoch 9/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 25ms/step - accuracy: 0.9346 - loss: 0.1709 - val_accuracy: 0.8915 - val_loss: 0.3356\n",
      "Epoch 10/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.9405 - loss: 0.1583 - val_accuracy: 0.8967 - val_loss: 0.3088\n",
      "Epoch 11/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9438 - loss: 0.1456 - val_accuracy: 0.8975 - val_loss: 0.3189\n",
      "Epoch 12/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9465 - loss: 0.1452 - val_accuracy: 0.8905 - val_loss: 0.3607\n",
      "Epoch 13/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9504 - loss: 0.1292 - val_accuracy: 0.8953 - val_loss: 0.3410\n",
      "Epoch 14/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9532 - loss: 0.1243 - val_accuracy: 0.8955 - val_loss: 0.3516\n",
      "Epoch 15/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9585 - loss: 0.1123 - val_accuracy: 0.8972 - val_loss: 0.3663\n",
      "Epoch 16/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9602 - loss: 0.1045 - val_accuracy: 0.9015 - val_loss: 0.3619\n",
      "Epoch 17/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9643 - loss: 0.0962 - val_accuracy: 0.8880 - val_loss: 0.4141\n",
      "Epoch 18/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9639 - loss: 0.0976 - val_accuracy: 0.8915 - val_loss: 0.4179\n",
      "Epoch 19/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9651 - loss: 0.0935 - val_accuracy: 0.8957 - val_loss: 0.4208\n",
      "Epoch 20/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.9701 - loss: 0.0805 - val_accuracy: 0.8970 - val_loss: 0.4209\n",
      "Epoch 21/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9682 - loss: 0.0857 - val_accuracy: 0.8968 - val_loss: 0.4260\n",
      "Epoch 22/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 26ms/step - accuracy: 0.9734 - loss: 0.0710 - val_accuracy: 0.8933 - val_loss: 0.4693\n",
      "Epoch 23/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.9740 - loss: 0.0691 - val_accuracy: 0.8952 - val_loss: 0.5134\n",
      "Epoch 24/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 26ms/step - accuracy: 0.9755 - loss: 0.0675 - val_accuracy: 0.8938 - val_loss: 0.5358\n",
      "Epoch 25/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 25ms/step - accuracy: 0.9784 - loss: 0.0625 - val_accuracy: 0.8915 - val_loss: 0.5476\n",
      "Epoch 26/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.9775 - loss: 0.0614 - val_accuracy: 0.8933 - val_loss: 0.5472\n",
      "Epoch 27/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9775 - loss: 0.0621 - val_accuracy: 0.8960 - val_loss: 0.6269\n",
      "Epoch 28/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9819 - loss: 0.0476 - val_accuracy: 0.8975 - val_loss: 0.5796\n",
      "Epoch 29/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9808 - loss: 0.0554 - val_accuracy: 0.8988 - val_loss: 0.5950\n",
      "Epoch 30/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9788 - loss: 0.0596 - val_accuracy: 0.8920 - val_loss: 0.6311\n",
      "Epoch 31/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9835 - loss: 0.0456 - val_accuracy: 0.8952 - val_loss: 0.6586\n",
      "Epoch 32/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9857 - loss: 0.0379 - val_accuracy: 0.8917 - val_loss: 0.7405\n",
      "Epoch 33/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9801 - loss: 0.0545 - val_accuracy: 0.8870 - val_loss: 0.7253\n",
      "Epoch 34/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 25ms/step - accuracy: 0.9840 - loss: 0.0472 - val_accuracy: 0.8920 - val_loss: 0.7029\n",
      "Epoch 35/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9854 - loss: 0.0414 - val_accuracy: 0.8975 - val_loss: 0.6923\n",
      "Epoch 36/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9879 - loss: 0.0368 - val_accuracy: 0.8925 - val_loss: 0.7179\n",
      "Epoch 37/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 25ms/step - accuracy: 0.9800 - loss: 0.0574 - val_accuracy: 0.8885 - val_loss: 0.7491\n",
      "Epoch 38/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.9854 - loss: 0.0429 - val_accuracy: 0.8953 - val_loss: 0.7322\n",
      "Epoch 39/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9878 - loss: 0.0330 - val_accuracy: 0.8925 - val_loss: 0.7173\n",
      "Epoch 40/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9877 - loss: 0.0374 - val_accuracy: 0.8960 - val_loss: 0.7727\n",
      "Epoch 41/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9881 - loss: 0.0337 - val_accuracy: 0.8918 - val_loss: 0.8096\n",
      "Epoch 42/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 25ms/step - accuracy: 0.9866 - loss: 0.0404 - val_accuracy: 0.8953 - val_loss: 0.8205\n",
      "Epoch 43/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 26ms/step - accuracy: 0.9893 - loss: 0.0327 - val_accuracy: 0.8928 - val_loss: 0.7955\n",
      "Epoch 44/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 26ms/step - accuracy: 0.9868 - loss: 0.0421 - val_accuracy: 0.8945 - val_loss: 0.9000\n",
      "Epoch 45/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 26ms/step - accuracy: 0.9891 - loss: 0.0325 - val_accuracy: 0.8898 - val_loss: 0.8706\n",
      "Epoch 46/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 25ms/step - accuracy: 0.9894 - loss: 0.0291 - val_accuracy: 0.8980 - val_loss: 0.7936\n",
      "Epoch 47/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 26ms/step - accuracy: 0.9889 - loss: 0.0364 - val_accuracy: 0.8977 - val_loss: 0.8238\n",
      "Epoch 48/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.9897 - loss: 0.0307 - val_accuracy: 0.8950 - val_loss: 0.8707\n",
      "Epoch 49/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 26ms/step - accuracy: 0.9900 - loss: 0.0292 - val_accuracy: 0.8967 - val_loss: 0.9657\n",
      "Epoch 50/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.9887 - loss: 0.0356 - val_accuracy: 0.8977 - val_loss: 0.9384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x19913379030>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size = (2,2)),\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size = (2,2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "md2.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "md2.fit(x_train, y_train, epochs = 50, batch_size = 128, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execise 2(d) (10 points)\n",
    "\n",
    "Build a CNN model as follows:\n",
    "\n",
    "- The CNN model should have the following layers in the given order:\n",
    "    - `Conv2D` with 128 filters, `kernel_size=(3,3)` and `activation=relu`\n",
    "    - `MaxPooling2D` with `pool_size=(2,2)`\n",
    "    - `Conv2D` with 128 filters, `kernel_size=(3,3)` and `activation=relu`\n",
    "    - `MaxPooling2D` with `pool_size=(2,2)`\n",
    "    - `Flatten`\n",
    "    - `Dense` with 128 neurons and `activation=relu`\n",
    "    - `Dense` with 10 neurons and `activation=softmax`\n",
    "- Compile the network with the following:\n",
    "    - `optimizer='adam'`\n",
    "    - `loss='categorical_crossentropy'`\n",
    "    - `metrics=['accuracy']`\n",
    "- Train the deep neural network with `epochs=50`, `batch_size=128`, and `validation_split=0.1`.\n",
    "- Evaluate the model on the `test` data set.\n",
    "\n",
    "Notice that there is no need to 0-1 encode the target labels; you can go ahead and train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gmgma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 74ms/step - accuracy: 0.6996 - loss: 3.9483 - val_accuracy: 0.8502 - val_loss: 0.4151\n",
      "Epoch 2/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 74ms/step - accuracy: 0.8729 - loss: 0.3462 - val_accuracy: 0.8785 - val_loss: 0.3356\n",
      "Epoch 3/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 73ms/step - accuracy: 0.8922 - loss: 0.2925 - val_accuracy: 0.8900 - val_loss: 0.2980\n",
      "Epoch 4/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 73ms/step - accuracy: 0.9028 - loss: 0.2606 - val_accuracy: 0.8930 - val_loss: 0.2943\n",
      "Epoch 5/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 74ms/step - accuracy: 0.9146 - loss: 0.2342 - val_accuracy: 0.8952 - val_loss: 0.2857\n",
      "Epoch 6/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 73ms/step - accuracy: 0.9191 - loss: 0.2179 - val_accuracy: 0.8760 - val_loss: 0.3377\n",
      "Epoch 7/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 73ms/step - accuracy: 0.9209 - loss: 0.2080 - val_accuracy: 0.8947 - val_loss: 0.2952\n",
      "Epoch 8/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 73ms/step - accuracy: 0.9285 - loss: 0.1907 - val_accuracy: 0.8943 - val_loss: 0.3051\n",
      "Epoch 9/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 73ms/step - accuracy: 0.9327 - loss: 0.1821 - val_accuracy: 0.8997 - val_loss: 0.2992\n",
      "Epoch 10/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 73ms/step - accuracy: 0.9344 - loss: 0.1670 - val_accuracy: 0.8927 - val_loss: 0.3115\n",
      "Epoch 11/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 74ms/step - accuracy: 0.9406 - loss: 0.1566 - val_accuracy: 0.9005 - val_loss: 0.3018\n",
      "Epoch 12/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 74ms/step - accuracy: 0.9432 - loss: 0.1463 - val_accuracy: 0.8932 - val_loss: 0.3209\n",
      "Epoch 13/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 74ms/step - accuracy: 0.9460 - loss: 0.1404 - val_accuracy: 0.8985 - val_loss: 0.3408\n",
      "Epoch 14/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 73ms/step - accuracy: 0.9515 - loss: 0.1269 - val_accuracy: 0.8967 - val_loss: 0.3415\n",
      "Epoch 15/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 74ms/step - accuracy: 0.9535 - loss: 0.1220 - val_accuracy: 0.8933 - val_loss: 0.3718\n",
      "Epoch 16/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 74ms/step - accuracy: 0.9550 - loss: 0.1194 - val_accuracy: 0.8948 - val_loss: 0.3823\n",
      "Epoch 17/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 74ms/step - accuracy: 0.9599 - loss: 0.1054 - val_accuracy: 0.8918 - val_loss: 0.4071\n",
      "Epoch 18/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 75ms/step - accuracy: 0.9608 - loss: 0.1045 - val_accuracy: 0.9000 - val_loss: 0.3925\n",
      "Epoch 19/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 79ms/step - accuracy: 0.9666 - loss: 0.0891 - val_accuracy: 0.8978 - val_loss: 0.4111\n",
      "Epoch 20/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 74ms/step - accuracy: 0.9681 - loss: 0.0878 - val_accuracy: 0.8950 - val_loss: 0.4497\n",
      "Epoch 21/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 74ms/step - accuracy: 0.9671 - loss: 0.0864 - val_accuracy: 0.8912 - val_loss: 0.4803\n",
      "Epoch 22/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 74ms/step - accuracy: 0.9716 - loss: 0.0752 - val_accuracy: 0.8960 - val_loss: 0.4804\n",
      "Epoch 23/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 74ms/step - accuracy: 0.9720 - loss: 0.0766 - val_accuracy: 0.8948 - val_loss: 0.4892\n",
      "Epoch 24/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 74ms/step - accuracy: 0.9684 - loss: 0.0861 - val_accuracy: 0.8943 - val_loss: 0.5071\n",
      "Epoch 25/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 74ms/step - accuracy: 0.9750 - loss: 0.0655 - val_accuracy: 0.8960 - val_loss: 0.4862\n",
      "Epoch 26/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 74ms/step - accuracy: 0.9766 - loss: 0.0632 - val_accuracy: 0.8963 - val_loss: 0.5591\n",
      "Epoch 27/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 74ms/step - accuracy: 0.9797 - loss: 0.0547 - val_accuracy: 0.8950 - val_loss: 0.6148\n",
      "Epoch 28/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 75ms/step - accuracy: 0.9758 - loss: 0.0693 - val_accuracy: 0.8852 - val_loss: 0.6313\n",
      "Epoch 29/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 75ms/step - accuracy: 0.9786 - loss: 0.0602 - val_accuracy: 0.8980 - val_loss: 0.5969\n",
      "Epoch 30/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 74ms/step - accuracy: 0.9830 - loss: 0.0477 - val_accuracy: 0.8887 - val_loss: 0.5476\n",
      "Epoch 31/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 74ms/step - accuracy: 0.9817 - loss: 0.0486 - val_accuracy: 0.8948 - val_loss: 0.6374\n",
      "Epoch 32/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 74ms/step - accuracy: 0.9811 - loss: 0.0538 - val_accuracy: 0.8983 - val_loss: 0.6633\n",
      "Epoch 33/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 74ms/step - accuracy: 0.9860 - loss: 0.0408 - val_accuracy: 0.8943 - val_loss: 0.6474\n",
      "Epoch 34/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 74ms/step - accuracy: 0.9845 - loss: 0.0452 - val_accuracy: 0.8928 - val_loss: 0.6703\n",
      "Epoch 35/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 74ms/step - accuracy: 0.9848 - loss: 0.0422 - val_accuracy: 0.8963 - val_loss: 0.7001\n",
      "Epoch 36/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 74ms/step - accuracy: 0.9876 - loss: 0.0351 - val_accuracy: 0.8985 - val_loss: 0.7502\n",
      "Epoch 37/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 74ms/step - accuracy: 0.9876 - loss: 0.0343 - val_accuracy: 0.8958 - val_loss: 0.7514\n",
      "Epoch 38/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 78ms/step - accuracy: 0.9879 - loss: 0.0376 - val_accuracy: 0.8973 - val_loss: 0.7173\n",
      "Epoch 39/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 76ms/step - accuracy: 0.9888 - loss: 0.0320 - val_accuracy: 0.8967 - val_loss: 0.7949\n",
      "Epoch 40/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 75ms/step - accuracy: 0.9876 - loss: 0.0353 - val_accuracy: 0.8973 - val_loss: 0.7833\n",
      "Epoch 41/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 75ms/step - accuracy: 0.9903 - loss: 0.0273 - val_accuracy: 0.8995 - val_loss: 0.8393\n",
      "Epoch 42/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 74ms/step - accuracy: 0.9830 - loss: 0.0532 - val_accuracy: 0.8952 - val_loss: 0.8244\n",
      "Epoch 43/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 74ms/step - accuracy: 0.9875 - loss: 0.0389 - val_accuracy: 0.8957 - val_loss: 0.7641\n",
      "Epoch 44/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 75ms/step - accuracy: 0.9868 - loss: 0.0396 - val_accuracy: 0.8937 - val_loss: 0.8355\n",
      "Epoch 45/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 74ms/step - accuracy: 0.9890 - loss: 0.0316 - val_accuracy: 0.8958 - val_loss: 0.8497\n",
      "Epoch 46/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 74ms/step - accuracy: 0.9902 - loss: 0.0321 - val_accuracy: 0.8987 - val_loss: 0.9127\n",
      "Epoch 47/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 74ms/step - accuracy: 0.9892 - loss: 0.0318 - val_accuracy: 0.8983 - val_loss: 0.8857\n",
      "Epoch 48/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 74ms/step - accuracy: 0.9922 - loss: 0.0259 - val_accuracy: 0.8963 - val_loss: 0.9035\n",
      "Epoch 49/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 74ms/step - accuracy: 0.9912 - loss: 0.0309 - val_accuracy: 0.8982 - val_loss: 0.9232\n",
      "Epoch 50/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 75ms/step - accuracy: 0.9897 - loss: 0.0292 - val_accuracy: 0.8953 - val_loss: 0.9430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x199140f75e0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md3 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(128, kernel_size=(3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size = (2,2)),\n",
    "    tf.keras.layers.Conv2D(128, kernel_size=(3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size = (2,2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "md3.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "md3.fit(x_train, y_train, epochs = 50, batch_size = 128, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8812999725341797\n",
      "0.8906999826431274\n",
      "0.890500009059906\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = md1.evaluate(x_test, y_test, verbose = 0)\n",
    "print(test_acc)\n",
    "test_loss, test_acc = md2.evaluate(x_test, y_test, verbose = 0)\n",
    "print(test_acc)\n",
    "test_loss, test_acc = md3.evaluate(x_test, y_test, verbose = 0)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2(e) (3 points)\n",
    "\n",
    "Based on the results from parts 2(b)-2(d), which model would you use to predict the label of the `fashion_mnist` data set? Please be specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results above I would use model 2 because it had the highest accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_547",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
